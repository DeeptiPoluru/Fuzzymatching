{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy matching \n",
    "\n",
    "input files: pv_disc_26Apr20.csv,crsp_disc_26Apr20.csv\n",
    "output files: fuzzymatched_crsp_pv_30Apr20.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctm = pd.read_csv(\"pv_disc_26Apr20.csv\")\n",
    "pv3 = pd.read_csv(\"crsp_disc_26Apr20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_letters = list(pv3['org_first'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fuzzy calculation for standardized clean names\n",
    "\n",
    "## use threshold score as 70\n",
    "\n",
    "dfc = pd.DataFrame({\"crsp\": ctm.comlow4.unique()})\n",
    "pv_letters = list(pv3.org_first.unique())\n",
    "pv_letters.reverse()\n",
    "for i in pv_letters:\n",
    "    print(i)\n",
    "    A = time.time()\n",
    "    \n",
    "    dfp = pd.DataFrame({\"pv\": pv3[pv3['org_first']==i]['orglow9'].unique()})\n",
    "    x = np.array(np.meshgrid(dfp.pv.values, dfc.crsp.values)).T.reshape(-1,2) # create matrix for pairwise fuzzy score\n",
    "    df3 = pd.DataFrame(x)\n",
    "    \n",
    "    df3.columns = ['pv','crsp']\n",
    "    \n",
    "    df3['Ratio'] = [fuzz.ratio(*i) for i in map(tuple, x)]\n",
    "    df3['Token'] = [fuzz.token_set_ratio(*i) for i in map(tuple, x)]\n",
    "    print(time.time()-A)\n",
    "    df3 = df3[(df3['Ratio']>70) | (df3['Token']>70)]\n",
    "    name = \"sub_\"+i+\".csv\"\n",
    "    df3.to_csv(name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## fuzzy calculation for new clean names (after removing high frequency last words)\n",
    "\n",
    "## use threshold score as 70\n",
    "\n",
    "dfc = pd.DataFrame({\"crsp_new\": ctm.comlow_new.unique()})\n",
    "\n",
    "pv_letters.reverse()\n",
    "for i in pv_letters:\n",
    "    print(i)\n",
    "    A = time.time()\n",
    "    \n",
    "    dfp = pd.DataFrame({\"pv_new\": pv3[pv3['org_first']==i]['orglow_new'].unique()})\n",
    "    x = np.array(np.meshgrid(dfp.pv_new.values, dfc.crsp_new.values)).T.reshape(-1,2)\n",
    "    df3 = pd.DataFrame(x)\n",
    "    \n",
    "    df3.columns = ['pv_new','crsp_new']\n",
    "    \n",
    "    df3['new_ratio'] = [fuzz.ratio(*i) for i in map(tuple, x)]\n",
    "    df3['new_token'] = [fuzz.token_set_ratio(*i) for i in map(tuple, x)]\n",
    "    df3['new_fpr'] = [fuzz.partial_ratio(*i) for i in map(tuple, x)]\n",
    "    \n",
    "    print(time.time()-A)\n",
    "    df3 = df3[(df3['new_ratio']>70) | (df3['new_token']>70) | (df3['new_fpr']>70)]\n",
    "    name = \"sub_new\"+i+\".csv\"\n",
    "    df3.to_csv(name, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### from fuzzy matrix input, filter matches based on the following rules:\n",
    "\n",
    "1. for a given pair, if max(fuzzy_ratio, fuzzy_token) == 100, retain the pair\n",
    "2. If there is more than one match for a company name which has max(fuzzy_ratio, fuzzy_token) == 100, then retain the match with higher fuzz_ratio \n",
    "3. for pairs where max(fuzzy_ratio, fuzzy_token) != 100:\n",
    "    - retain all the names which have only one single match\n",
    "    - for names which have more than one match, retain the match with higher max(fuzzy_ratio, fuzzy_token)\n",
    "    - for names which have more than one match with same max(fuzzy_ratio, fuzzy_token), use higher ratio to break the tie\n",
    "4. finally of all the retained pairs, select only those with max(fuzzy_ratio, fuzzy_token) >70\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pv_letters:\n",
    "\n",
    "    ####for clean company names after removing high frequency last words \n",
    "    name = \"sub_new\"+i+\".csv\"\n",
    "    newe = pd.read_csv(name)\n",
    "    newe['mx'] = newe.apply(lambda x: max(x.new_ratio, x.new_token), axis = 1)\n",
    "\n",
    "\n",
    "    newe100 = newe[newe['mx']==100]  #for a given pair, if max(fuzzy_ratio, fuzzy_token) == 100, retain the pair\n",
    "    newe12 = newe100.drop_duplicates('pv_new', keep = False) ##retain- add to a list at the end\n",
    "    newe1 = newe100[newe100.duplicated('pv_new', keep = False)].reset_index() ##if tie, use ratio score to break it\n",
    "\n",
    "    newe2 =  newe1.loc[newe1.groupby(['pv_new'])['mx'].idxmax()]\n",
    "    idx = newe2.groupby(['pv_new'], sort=False)['new_ratio'].transform(max) == newe2['new_ratio']\n",
    "    newe3 = newe2[idx].sort_values(['pv_new']) ##to add at the end\n",
    "\n",
    "    newe33 = newe1[newe1.duplicated(['pv_new','new_ratio','new_token'], keep = False)] ## remove the matches already retained\n",
    "\n",
    "\n",
    "\n",
    "    ###not 100\n",
    "    newetemp = newe[~(newe['mx']==100)]\n",
    "    newe123 = newetemp.drop_duplicates('pv_new', keep = False) ## retain all the names which have only one single match to add to list at the end\n",
    "    newe1t = newetemp[newetemp.duplicated('pv_new', keep = False)] # remove matches already retained\n",
    "\n",
    "    new2 =  newe1t.loc[newe1t.groupby(['pv_new'])['mx'].idxmax()] # for names which have more than one match, retain the match with higher max(fuzzy_ratio, fuzzy_token)\n",
    "    idx2 = new2.groupby(['pv_new'], sort=False)['mx'].transform(max) == new2['mx']\n",
    "    new3 = new2[idx2].sort_values(['pv_new']) ### contains some duplicates in pv\n",
    "\n",
    "    newe4 =  new3.loc[new3.groupby(['pv_new'])['new_ratio'].idxmax()] ## use ratio to break tie\n",
    "    idx3 = newe4.groupby(['pv_new'], sort=False)['new_ratio'].transform(max) == newe4['new_ratio']\n",
    "    new5 = newe4[idx3].sort_values('pv_new') ##add\n",
    "\n",
    "    new5 = new5.append(newe12)\n",
    "    new6 = new5.append(newe3)\n",
    "    new7 = new6.append(newe123)\n",
    "    new8 = new7.append(newe33)\n",
    "    new9 = new8[new8['mx']>70]\n",
    "\n",
    "    new9 = new9.drop_duplicates(keep = 'first')\n",
    "    \n",
    "    ######################################################\n",
    "    \n",
    "     ####for clean company names after standardizing\n",
    "    lname = \"sub_\"+i+\".csv\"\n",
    "    lowe = pd.read_csv(lname)\n",
    "    lowe['mx'] = lowe.apply(lambda x: max(x.Ratio, x.Token), axis = 1)\n",
    "\n",
    "    ##100\n",
    "    \n",
    "    lowe100 = lowe[lowe['mx']==100]\n",
    "    lowe12 = lowe100.drop_duplicates('pv', keep = False) ##to add at the end\n",
    "    lowe1 = lowe100[lowe100.duplicated('pv', keep = False)].reset_index() ##duplicates based on same pv\n",
    "\n",
    "    lowe33 = lowe1[lowe1.duplicated(['pv','Ratio','Token'], keep = False)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lowe2 =  lowe1.loc[lowe1.groupby(['pv'])['mx'].idxmax()]\n",
    "    idx = lowe2.groupby(['pv'], sort=False)['Ratio'].transform(max) == lowe2['Ratio']\n",
    "    lowe3 = lowe2[idx].sort_values(['pv']) ##to add at the end\n",
    "\n",
    "    ###not 100\n",
    "\n",
    "    lowetemp = lowe[~(lowe['mx']==100)]\n",
    "    lowe123 = lowetemp.drop_duplicates('pv', keep = False) ##to add at the end\n",
    "    lowe1t = lowetemp[lowetemp.duplicated('pv', keep = False)]\n",
    "\n",
    "    low2 =  lowe1t.loc[lowe1t.groupby(['pv'])['mx'].idxmax()]\n",
    "    idx2 = low2.groupby(['pv'], sort=False)['mx'].transform(max) == low2['mx']\n",
    "    low3 = low2[idx2].sort_values(['pv'])  #of these some have multiple\n",
    "\n",
    "    lowe4 =  low3.loc[low3.groupby(['pv'])['Ratio'].idxmax()]\n",
    "    idx3 = lowe4.groupby(['pv'], sort=False)['Ratio'].transform(max) == lowe4['Ratio']\n",
    "    low5 = lowe4[idx3].sort_values('pv') ##add\n",
    "    \n",
    "    ##if duplicates, retain one with same starting string\n",
    "\n",
    "\n",
    "    low5 = low5.append(lowe12)\n",
    "    low6 = low5.append(lowe3)\n",
    "    low7 = low6.append(lowe123)\n",
    "    low8 = low7.append(lowe33)\n",
    "    low9 = low8[low8['mx']>70]\n",
    "\n",
    "    low9 = low9.drop_duplicates(keep = 'first')\n",
    "\n",
    "#####################################################################\n",
    "    ####join with crsp##\n",
    "    df_new =  pd.merge(new9, ctm, how = \"left\",left_on = ['crsp_new'], right_on = ['comlow_new'])\n",
    "    df_low =  pd.merge(low9, ctm, how = \"left\",left_on = ['crsp'], right_on = ['comlow4'])\n",
    "\n",
    "    ##remove duplicates###\n",
    "    df_new = df_new.drop_duplicates(keep = 'first')\n",
    "    df_low = df_low.drop_duplicates(keep = 'first')\n",
    "    \n",
    "    ###merge with pv###\n",
    "    \n",
    "    pv_sub = pv3[pv3['org_first']==i]\n",
    "    pv_sub = pv_sub[['assignee_id', 'organization', 'orglow9','orglow_new', 'org_first']]\n",
    "    pv_sub = pv_sub.drop_duplicates(keep = 'first')\n",
    "    \n",
    "    \n",
    "    pc_new = pd.merge(pv_sub,df_new, left_on = ['orglow_new'], right_on = ['pv_new'], how = \"left\")\n",
    "    pc_low = pd.merge(pv_sub,df_low,  left_on = ['orglow9'], right_on = ['pv'], how = \"left\")\n",
    "    \n",
    "    \n",
    "    # remove na\n",
    "    pc_new = pc_new[~pc_new['pv_new'].isna()]\n",
    "    pc_low = pc_low[~pc_low['pv'].isna()]\n",
    "    \n",
    "    pc_new2 = pc_new[['assignee_id', 'organization', 'orglow9', 'orglow_new',\n",
    "       'pv_new', 'COMNAM','crsp_new',  'comlow4', 'comlow_new',\n",
    "                  'new_ratio', 'new_token', 'new_fpr','PERMNO']]\n",
    "    pc_low2 = pc_low[['assignee_id', 'organization', 'orglow9', 'orglow_new','pv', 'crsp', 'Ratio', 'Token',\n",
    "                 'PERMNO', 'COMNAM','comlow4','comlow_new']]\n",
    "    \n",
    "    ###join as one data frame\n",
    "\n",
    "    pc = pd.merge(pc_new2, pc_low2, how = \"outer\",  \n",
    "              left_on =['assignee_id', 'organization', 'orglow9', 'orglow_new', 'COMNAM','PERMNO',\n",
    "                       'comlow4','comlow_new'],\n",
    "             right_on = ['assignee_id', 'organization', 'orglow9', 'orglow_new', 'COMNAM','PERMNO',\n",
    "                        'comlow4','comlow_new'])\n",
    "    \n",
    "    pc['Ratio'] = pc.apply(lambda x: fuzz.ratio(x.comlow4, x.orglow9), axis = 1)\n",
    "    pc['Token'] = pc.apply(lambda x: fuzz.token_set_ratio(x.comlow4, x.orglow9), axis = 1)\n",
    "    pc['new_ratio'] = pc.apply(lambda x: fuzz.ratio(x.comlow_new, x.orglow_new), axis = 1)\n",
    "    pc['new_token'] = pc.apply(lambda x: fuzz.token_set_ratio(x.comlow_new, x.orglow_new), axis = 1)\n",
    "    pc['new_fpr'] = pc.apply(lambda x: fuzz.partial_ratio(x.comlow_new, x.orglow_new), axis = 1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute max of top two scores. after trial and error,\n",
    "\n",
    "## after testing variations of  average score, linear combination of various scores, I figured that this\n",
    "# measure gives good results\n",
    "## and can be used at varying threshold to calculate precision and recall to get good matches\n",
    "\n",
    "pc['top2'] = pc.apply(lambda x: statistics.mean(sorted([x.new_ratio,x.new_token,x.Token,x.Ratio])[2:]),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##compute average of fuzzy scores after removing high freq last words\n",
    "\n",
    "pc['avg_low'] = pc.apply(lambda x: statistics.mean(([x.new_ratio,x.new_token])),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##compute average of fuzzy scores after standardizing and cleaning names\n",
    "\n",
    "pc['avg_new'] = pc.apply(lambda x: statistics.mean(([x.Ratio,x.Token])),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average of all scores\n",
    "pc['avg'] = pc.apply(lambda x: statistics.mean(sorted([x.new_ratio,x.new_token,x.Token,x.Ratio])),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use manual testing to check if proper match or not##\n",
    "## create \"finalmerge\" column to mark if match == True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.to_csv(\"fuzzymatched_crsp_pv_30Apr20.csv\", index = False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
